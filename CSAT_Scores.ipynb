{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keerthana-narra/csat_score_prediction_dl/blob/main/CSAT_Scores.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILM16xV1PD8m"
      },
      "source": [
        "# **Customer Satisfaction Scores using Deep Learning**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqVZBxrvPD8n"
      },
      "source": [
        "##### **Project Type**    - Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQBIQOjeO5eC"
      },
      "source": [
        "**Github link :** https://github.com/keerthana-narra/CSAT-score-prediction-DL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5Fh4-5QjziR"
      },
      "source": [
        "Project Documentation : https://docs.google.com/document/d/1bBNIqykhKehADVZ0dzgzPOmfkhAEQOwm/edit?usp=drive_link&ouid=101693464124780926449&rtpof=true&sd=true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbzKaGcJO0b9"
      },
      "source": [
        "Presentation : [Video link]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQaldy8SH6Dl"
      },
      "source": [
        "# **Problem Statement and Objective**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jivUA-ONOwse"
      },
      "source": [
        "**Problem Overview**\n",
        "\n",
        "This project focuses on predicting Customer Satisfaction (CSAT) scores using Deep Learning Artificial Neural Networks (ANN). In the context of e-commerce, understanding customer satisfaction through their interactions and feedback is crucial for enhancing service quality, customer retention, and overall business growth.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmr4zSC6BYa0"
      },
      "source": [
        "**Objective:**\n",
        "\n",
        " The goal of project is to leverage advanced neural network models, to forecast CSAT scores based on a myriad of interaction-related features, providing actionable insights for service improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_i_v8NEhb9l"
      },
      "source": [
        "# ***Let's Begin !***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhfV-JJviCcP"
      },
      "source": [
        "## ***1. Know Your Data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okemS-mzEBS0"
      },
      "source": [
        "Before we diving into the project, our initial step is to familiarize ourselves with the dataset. Let's load the data and take a look."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3lxredqlCYt"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8Vqi-pPk-HR",
        "outputId": "745a3903-529f-454f-8a43-437f42b000d2"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt\n",
        "\n",
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from category_encoders.target_encoder import TargetEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from scipy.stats import uniform, randint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RnN4peoiCZX"
      },
      "source": [
        "### Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CkvbW_SlZ_R",
        "outputId": "02cbce4e-6829-46cb-d69f-7a9aa4321212"
      },
      "outputs": [],
      "source": [
        "# #Mount drive and Load data\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "#df=pd.read_csv('/content/drive/MyDrive/Almabetter/Masters/Specilization/Module4/Capstone/data/eCommerce_Customer_support_data.csv')\n",
        "\n",
        "#Load the dataset\n",
        "df = pd.read_csv('data/eCommerce_Customer_support_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPPbIbDGXOfO"
      },
      "outputs": [],
      "source": [
        "# Change column names to lowercase with underscores instead of spaces\n",
        "df.columns = df.columns.str.lower().str.replace(' ', '_')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x71ZqKXriCWQ"
      },
      "source": [
        "### Peek into data ðŸ‘€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWNFOSvLl09H",
        "outputId": "55e7291d-54e1-4357-abb0-d611e142564d"
      },
      "outputs": [],
      "source": [
        "#First 5 rows\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsD4L_hZxwPJ",
        "outputId": "302b33aa-6bac-4090-d97b-072ac0382bb2"
      },
      "outputs": [],
      "source": [
        "#Last 5 rows\n",
        "df.tail(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kllu7SJgmLij",
        "outputId": "a0f04561-0b66-4516-9993-02172a0d6672"
      },
      "outputs": [],
      "source": [
        "# Dataset Rows & Columns\n",
        "print(f'Shape of original dataframe:  {df.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      },
      "source": [
        "## ***2. Understanding Data & Preprocessing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsBHh7p5GBxE"
      },
      "source": [
        "Moving forward to the next stage, our focus is on gaining a deeper understanding of the data and undertaking the initial preprocessing steps. These initial preprocessing efforts are crucial for enhancing the usability of the data in subsequent analyses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKaaJyKJ5dUD"
      },
      "source": [
        "### **Understanding Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R676N4wX3anB",
        "outputId": "1279a945-459a-4c92-b130-2dc0c72bd9ec"
      },
      "outputs": [],
      "source": [
        "#Variables in the dataset\n",
        "print(f'Variables in the dataset : {list(df.columns)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBTbrJXOngz2"
      },
      "source": [
        "**Variables Description**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJV4KIxSnxay"
      },
      "source": [
        "* **Unique id:** Unique identifier for each record (integer).\n",
        "* **Channel name:** Name of the customer service channel (object/string).\n",
        "* **Category:** Category of the interaction (object/string).\n",
        "* **Sub-category:** Sub-category of the interaction (object/string).\n",
        "* **Customer Remarks:** Feedback provided by the customer (object/string).\n",
        "* **Order id:** Identifier for the order associated with the interaction (integer).\n",
        "* **Order date time:** Date and time of the order (datetime).\n",
        "* **Issue reported at:** Timestamp when the issue was reported (datetime).\n",
        "* **Issue responded:** Timestamp when the issue was responded to (datetime).\n",
        "* **Survey response date:** Date of the customer survey response (datetime).\n",
        "* **Customer city:** City of the customer (object/string).\n",
        "* **Product category:** Category of the product (object/string).\n",
        "* **Item price:** Price of the item (float).\n",
        "* **Connected handling time:** Time taken to handle the interaction (float).\n",
        "* **Agent name:** Name of the customer service agent (object/string).\n",
        "* **Supervisor:** Name of the supervisor (object/string).\n",
        "* **Manager:** Name of the manager (object/string).\n",
        "* **Tenure Bucket:** Bucket categorizing agent tenure (object/string).\n",
        "* **Agent Shift:** Shift timing of the agent (object/string).\n",
        "* **CSAT Score:** Customer Satisfaction (CSAT) score (integer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iW4ZYkgqazgx"
      },
      "outputs": [],
      "source": [
        "def summary(df):\n",
        "    summry = pd.DataFrame(df.dtypes, columns=['Data type'])\n",
        "    summry['#Missing'] = df.isnull().sum().values\n",
        "    summry['#Duplicate'] = df.duplicated().sum()\n",
        "    summry['#Unique'] = df.nunique().values\n",
        "    desc = pd.DataFrame(df.describe(include='all').transpose())\n",
        "    summry['Min'] = desc['min'].values\n",
        "    summry['Max'] = desc['max'].values\n",
        "    summry['Avg'] = desc['mean'].values\n",
        "    summry['Std dev'] = desc['std'].values\n",
        "    summry['Top value'] = desc['top'].values\n",
        "    summry['Freq'] = desc['freq'].values\n",
        "\n",
        "    return summry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjK5c6CXa4iS",
        "outputId": "a66ef646-55e1-45c3-d79e-a502ab10ae36"
      },
      "outputs": [],
      "source": [
        "summary(df).style.set_caption(\"<b style='font-size:16px;'>DATA SUMMARY</b>\").\\\n",
        "background_gradient(cmap='Blues', axis=0). \\\n",
        "set_properties(**{'border': '1.3px dotted', 'color': '', 'caption-side': 'left'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHTaonPCXIuF",
        "outputId": "389bfdff-9de6-4d97-b064-16e7c59608d7"
      },
      "outputs": [],
      "source": [
        "def print_unique_counts(df, columns):\n",
        "    for col in columns:\n",
        "        unique_count = df[col].nunique()\n",
        "        value_counts = df[col].value_counts()\n",
        "        print(f\"There are {unique_count} unique values and counts of each in {value_counts}\\n\")\n",
        "\n",
        "# Columns to check uniques and values\n",
        "columns_to_print = ['channel_name', 'category', 'manager', 'tenure_bucket', 'agent_shift', 'csat_score']\n",
        "print_unique_counts(df, columns_to_print)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3ttC5nJXkQP",
        "outputId": "1efbc86e-1d99-43a2-f266-8c4815bb657a"
      },
      "outputs": [],
      "source": [
        "# Understanding categories and sub-categories\n",
        "grouped_counts = df.groupby(['category', 'sub-category']).size().reset_index(name='count')\n",
        "print(grouped_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Here by its been understood that there are huge number of missing values in 'customer_remarks', 'order_id', 'order_date_time',  'customer_city', 'product_category', 'item_price', 'connected_handling_time'\n",
        "\n",
        "* Category, Sub-category are the columns which tells about the what the query is related to.\n",
        "\n",
        "* Manager(6) -> Supervisor(40) -> Agent(1371) hirerachy is present which can give insight about the performance of employees\n",
        "\n",
        "* CSAT score is ranging from 1 to 5. With average as 4.2 This tells that the customer support overall performance as Excellent. However lets deep dive and get the action items for the team to cross more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMxWytdT5oEO"
      },
      "source": [
        "### ***Data Preprocessing***\n",
        "This step before EDA helps us to understand data easily further\n",
        "1. Drop the unique identifier and check for duplicates. As the unique identifier is a row generator.\n",
        "2. Date type conversion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIuM_FJcX37u"
      },
      "outputs": [],
      "source": [
        "# This function helps us to check and drop duplicates whenever required\n",
        "def check_drop_duplications(df):\n",
        "  if len(df[df.duplicated()]) > 0:\n",
        "    print(f'Count of duplicate rows : {len(df[df.duplicated()])}')\n",
        "    print(f'Dropping duplicates')\n",
        "    df = df.drop_duplicates()\n",
        "  else:\n",
        "    print(f'There are no duplicates.')\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUQZDwcmYDJ2"
      },
      "outputs": [],
      "source": [
        "# Dropping identifiers\n",
        "df = df.drop(columns = ['unique_id','order_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXWO-Y_ZVKk_",
        "outputId": "4605a2b4-b777-47dd-beb0-75397ff90953"
      },
      "outputs": [],
      "source": [
        "# Lets see if we have any duplicate rows\n",
        "df = check_drop_duplications(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4vi4xD-7QCv"
      },
      "outputs": [],
      "source": [
        "# Converting datetime columns to same format\n",
        "df['order_date_time'] = pd.to_datetime(df['order_date_time'])\n",
        "df['issue_reported_at'] = pd.to_datetime(df['issue_reported_at'], format='mixed', dayfirst=True)\n",
        "df['issue_responded'] = pd.to_datetime(df['issue_responded'], format='mixed', dayfirst=True)\n",
        "df['survey_response_date'] = pd.to_datetime(df['survey_response_date'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF8Ens_Soomf"
      },
      "source": [
        "## ***3. Exploratory Data Analysis***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Analysis and Charts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6mR8astPv44"
      },
      "source": [
        "#### Chart-1 CSAT score distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2NacykbOOM-",
        "outputId": "01e80043-c60f-45f6-dbed-ffc4348b28e7"
      },
      "outputs": [],
      "source": [
        "# Pie chart for CSAT score distribution\n",
        "csat_counts = df['csat_score'].value_counts()\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.pie(csat_counts, labels=csat_counts.index, autopct='%1.0f%%', startangle=140)\n",
        "plt.title('CSAT Score Distribution', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKgi_OAnCsy9"
      },
      "source": [
        "#### Chart - 2 CSAT score distibution by category\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpQtOiF2EoUN",
        "outputId": "f96adc13-95f0-4e62-f0b6-1c5ecb41228c"
      },
      "outputs": [],
      "source": [
        "# Calculate percentage of each CSAT score within each category\n",
        "category_csat_counts = df.groupby(['category', 'csat_score']).size().unstack(fill_value=0)\n",
        "category_csat_percentages = category_csat_counts.div(category_csat_counts.sum(axis=1), axis=0) * 100\n",
        "\n",
        "# Plot stacked bar chart with percentages\n",
        "custom_colors = {1: 'red', 5: 'green', 2: 'orange', 3: 'yellow', 4: 'blue'}\n",
        "# Plot stacked bar chart with custom colors\n",
        "category_csat_percentages.plot(kind='bar', stacked=True, figsize=(30, 6), color=[custom_colors[col] for col in category_csat_percentages.columns])\n",
        "# Customize plot labels and legend\n",
        "plt.title('CSAT Score Distribution by Category (Percentage)', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Percentage (%)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='CSAT Score', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate percentage of each CSAT score within each category\n",
        "category_csat_counts = df.groupby(['sub-category', 'csat_score']).size().unstack(fill_value=0)\n",
        "category_csat_percentages = category_csat_counts.div(category_csat_counts.sum(axis=1), axis=0) * 100\n",
        "\n",
        "# Plot stacked bar chart with custom colors\n",
        "category_csat_percentages.plot(kind='bar', stacked=True, figsize=(30, 6), color=[custom_colors[col] for col in category_csat_percentages.columns])\n",
        "# Customize plot labels and legend\n",
        "plt.title('CSAT Score Distribution by Sub-Category (Percentage)', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.xlabel('Sub Category')\n",
        "plt.ylabel('Percentage (%)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='CSAT Score', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Highlight categories where CSAT score 5 > 80%\n",
        "categories_high_csat1 = category_csat_percentages[category_csat_percentages[5] > 80].index\n",
        "\n",
        "# Print categories where CSAT score 5 exceeds 80% along with percentages\n",
        "print(f\"Categories where CSAT score 5 exceeds 80%:\")\n",
        "for category in categories_high_csat1:\n",
        "    percentage = category_csat_percentages.loc[category, 5]\n",
        "    print(f\"- {category}: {percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tT7VbNxJmgWr",
        "outputId": "67b7afb3-d4fe-4838-c3ff-bcdf627ffc1b"
      },
      "outputs": [],
      "source": [
        "# Highlight categories where CSAT score 1 > 20%\n",
        "categories_high_csat1 = category_csat_percentages[category_csat_percentages[1] > 20].index\n",
        "\n",
        "# Print categories where CSAT score 1 exceeds 20% along with percentages\n",
        "print(f\"Categories where CSAT score 1 exceeds 20%:\")\n",
        "for category in categories_high_csat1:\n",
        "    percentage = category_csat_percentages.loc[category, 1]\n",
        "    print(f\"- {category}: {percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Chart 3 - CSAT score distribution with customer remarks value presence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing or empty customer reviews\n",
        "missing_reviews = df['customer_remarks'].isna() | (df['customer_remarks'].str.strip() == '')\n",
        "\n",
        "# CSAT score distribution by presence of customer reviews\n",
        "csat_with_reviews = df[~missing_reviews]['csat_score'].value_counts().sort_index()\n",
        "csat_without_reviews = df[missing_reviews]['csat_score'].value_counts().sort_index()\n",
        "\n",
        "# Combine the data into a single DataFrame for stacked plotting\n",
        "csat_combined = pd.DataFrame({\n",
        "    'With Reviews': csat_with_reviews,\n",
        "    'Without Reviews': csat_without_reviews\n",
        "}).fillna(0)\n",
        "\n",
        "# Plotting the stacked bar chart\n",
        "csat_combined.plot(kind='bar', stacked=True, color=['green', 'red'], figsize=(10, 6))\n",
        "\n",
        "# Customize plot labels and title\n",
        "plt.title('CSAT Score Distribution (With vs. Without Reviews)', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.xlabel('CSAT Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title='Customer Reviews')\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evidence if the customer are dissatified >50% of them give remarks.\n",
        "csat_combined"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_hA8tsjvLSD"
      },
      "source": [
        "#### Chart 4 - Sentiment score Distribution and CSAT score distribution with sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gq8HI-g-4WxO"
      },
      "outputs": [],
      "source": [
        "# Initialize VADER\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to calculate sentiment score\n",
        "def calculate_sentiment(remark):\n",
        "    scores = sid.polarity_scores(remark)\n",
        "    compound_score = scores['compound']\n",
        "    return compound_score\n",
        "\n",
        "# Imputing mode in customer_remarks and with supporting insight from chart-3\n",
        "df['customer_remarks'] = df['customer_remarks'].fillna('good')\n",
        "# Apply sentiment calculation\n",
        "df['sentiment'] = df['customer_remarks'].apply(calculate_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfzp1kq-vLSb",
        "outputId": "a8b85c1e-a5c5-4b34-e968-1f21b1db3120"
      },
      "outputs": [],
      "source": [
        "# Set up figure with subplots\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
        "\n",
        "# Plot 1: Sentiment Score Distribution\n",
        "sns.histplot(df[df['sentiment'] > 0]['sentiment'], bins=20, kde=True, color='blue', ax=axes[0], label='Positive Sentiment')\n",
        "sns.histplot(df[df['sentiment'] < 0]['sentiment'], bins=20, kde=True, color='red', ax=axes[0], label='Negative Sentiment')\n",
        "axes[0].set_title('Sentiment Score Distribution')\n",
        "axes[0].set_xlabel('Sentiment Score')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].legend()\n",
        "\n",
        "# Plot 2: Sentiment Score with CSAT Distribution\n",
        "# Positive sentiment scores\n",
        "sns.histplot(df[df['sentiment'] > 0]['csat_score'], bins=5, kde=True, color='blue', alpha=0.5, ax=axes[1], label='Positive Sentiment')\n",
        "# Negative sentiment scores\n",
        "sns.histplot(df[df['sentiment'] < 0]['csat_score'], bins=5, kde=True, color='red', alpha=0.5, ax=axes[1], label='Negative Sentiment')\n",
        "axes[1].set_title('CSAT Score Distribution')\n",
        "axes[1].set_xlabel('CSAT Score')\n",
        "axes[1].set_ylabel('Count')\n",
        "axes[1].legend()\n",
        "\n",
        "# Adjust layout and display\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQY6D22Vyloa"
      },
      "source": [
        "#### Chart 5 - Average CSAT Score by Issue Response Date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxClqppRylob",
        "outputId": "3bdf7881-f7d2-49f8-ad22-655a556094ec"
      },
      "outputs": [],
      "source": [
        "# Aggregate CSAT scores by issue_responded date\n",
        "csat_by_date = df.groupby(df['issue_responded'].dt.date)['csat_score'].mean()\n",
        "\n",
        "# Plotting the line chart\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(csat_by_date.index, csat_by_date.values, linestyle='-', color='blue')\n",
        "\n",
        "plt.title('Average CSAT Score by Issue Response Date')\n",
        "plt.xlabel('Issue Response Date')\n",
        "plt.ylabel('Average CSAT Score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOX5_u4bF83_"
      },
      "source": [
        "#### Chart - 6 CSAT score distribution with Manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-ingsuUF8id",
        "outputId": "0ca92804-301d-4ef8-950a-46da58c72dab"
      },
      "outputs": [],
      "source": [
        "# Calculate percentage of each CSAT score within each category\n",
        "manager_csat_counts = df.groupby(['manager', 'csat_score']).size().unstack(fill_value=0)\n",
        "manager_csat_percentages = manager_csat_counts.div(manager_csat_counts.sum(axis=1), axis=0) * 100\n",
        "\n",
        "# Plot stacked bar chart with percentages\n",
        "custom_colors = {1: 'red', 5: 'green', 2: 'orange', 3: 'yellow', 4: 'blue'}\n",
        "# Plot stacked bar chart with custom colors\n",
        "manager_csat_percentages.plot(kind='bar', stacked=True, figsize=(30, 6), color=[custom_colors[col] for col in manager_csat_percentages.columns])\n",
        "# Customize plot labels and legend\n",
        "plt.title('CSAT Score Distribution by Manager (Percentage)', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.xlabel('Manager')\n",
        "plt.ylabel('Percentage (%)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='CSAT Score', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diOsR08WT_IJ"
      },
      "source": [
        "#### Chart 7 - Customer Reviews Word Cloud\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3bR0oBrUjY8",
        "outputId": "1dd0fbfa-6d6b-4346-cd81-f39581db9d7c"
      },
      "outputs": [],
      "source": [
        "# Combine all customer remarks into a single text string\n",
        "text = ' '.join(df['customer_remarks'].dropna())\n",
        "\n",
        "# Remove the word \"good\" (case insensitive)\n",
        "text = text.lower().replace('good', '')\n",
        "text = text.lower().replace('shopzilla', '')\n",
        "\n",
        "\n",
        "# Generate the word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "# Plot the word cloud\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.title('Customer Reviews Word Cloud')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Chart 8 - Agent performance\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the average CSAT score for each agent\n",
        "agent_performance = df.groupby('agent_name')['csat_score'].mean().reset_index()\n",
        "\n",
        "# Sort the agents by average CSAT score to find top and least performers\n",
        "top_performers = agent_performance.sort_values(by='csat_score', ascending=False).head()\n",
        "least_performers = agent_performance.sort_values(by='csat_score', ascending=True).head()\n",
        "\n",
        "# Display the top performers\n",
        "print(\"Top Performers:\")\n",
        "print(top_performers.to_string(index=False))\n",
        "\n",
        "# Display the least performers\n",
        "print(\"\\nLeast Performers:\")\n",
        "print(least_performers.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the median CSAT score for each agent\n",
        "agent_performance = df.groupby('agent_name')['csat_score'].median().reset_index()\n",
        "\n",
        "# Check which agents have a median CSAT score less than 3\n",
        "agents_below_3 = agent_performance[agent_performance['csat_score'] < 3]\n",
        "\n",
        "# Display the agents with a median CSAT score less than 3\n",
        "print(\"Agents with a median CSAT score less than 3:\")\n",
        "print(agents_below_3.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NLej2X2ctdG"
      },
      "source": [
        "### **Overall Insights and Recommendations from EDA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mSW6zdPc78F"
      },
      "source": [
        "***EDA INSIGHTS***\n",
        "\n",
        "**1. CSAT Rating Distribution:**\n",
        "\n",
        "A significant 70% of customer calls receive a perfect rating of 5, indicating a strong overall satisfaction with customer support. However, 13% of calls are rated at 4, which is still positive but suggests room for improvement. Notably, another 13% of calls are rated as 1, reflecting severe dissatisfaction. To elevate customer support from \"Excellent\" to \"Outstanding,\" it is crucial to minimize the number of calls receiving a rating of 1. Focused efforts on identifying and addressing the root causes of these low ratings will be essential.\n",
        "\n",
        "**2. CSAT Score Distribution by Category and Sub-Category:**\n",
        "\n",
        "Analyzing CSAT scores across different categories and sub-categories reveals that certain areas are prone to lower ratings. Categories like commission-related issues, account-related problems, and call disconnections have a concerning percentage of ratings at 1. In some sub-categories, more than 30% of interactions are rated as 1. On the other hand, sub-categories such as billing & payment, instant discount, non-order related issues, and app-related problems consistently receive ratings of 5, with over 80% of customers expressing high satisfaction. This highlights the need to target specific areas with tailored interventions to reduce dissatisfaction while continuing to reinforce the strengths of high-performing categories.\n",
        "\n",
        "**3. Customer Remarks and CSAT Scores:**\n",
        "\n",
        "Customer remarks are provided in about 50% of interactions, predominantly when customers are either highly satisfied or highly disappointed. This pattern suggests that remarks are a strong indicator of customer sentiment. Given that 'good' is the most common remark, it is recommended to impute 'good' in cases where no remark is provided. This approach will maintain data consistency and align with the observed mode in customer feedback.\n",
        "\n",
        "**4. Trend in CSAT Scores:**\n",
        "\n",
        "There is an observable upward trend in CSAT scores, indicating an overall improvement in customer satisfaction over time. This positive trajectory should be monitored closely, and efforts should continue to maintain and accelerate this growth, particularly by addressing areas of persistent dissatisfaction.\n",
        "\n",
        "**5. Manager Performance:**\n",
        "\n",
        "The analysis shows that all managers maintain a similar level of CSAT scores, suggesting a consistent performance across leadership. While this consistency is positive, it also indicates that there may be opportunities for innovation and differentiation in management practices to further enhance customer satisfaction.\n",
        "\n",
        "**6. Focus on Low-Performing Agents:**\n",
        "\n",
        "There are 20 agents with a median CSAT score of less than 3. This group represents a critical area for improvement. Implementing targeted training programs and conducting detailed performance reviews could help these agents address specific challenges and elevate their performance. Addressing the issues faced by these agents will be key to improving overall customer satisfaction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Recommendations:**\n",
        "1. Targeted Interventions: Focus on the categories and sub-categories with high percentages of low ratings (1s) to identify and resolve underlying issues.\n",
        "2. Training and Development: Provide additional support and training to the 20 agents with a median CSAT score of less than 3 to improve their performance.\n",
        "3. Proactive Customer Engagement: Leverage the upward trend in CSAT scores to explore new ways of exceeding customer expectations, especially in high-performing areas like billing and app-related issues.\n",
        "4. Continuous Monitoring: Maintain a consistent review of CSAT scores across categories, sub-categories, and agents to ensure that improvements are sustained and areas of dissatisfaction are promptly addressed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLjJCtPM0KBk"
      },
      "source": [
        "## ***4. Data Transformation***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiyOF9F70UgQ"
      },
      "source": [
        "### 1. Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUi6mdVI4DLx"
      },
      "outputs": [],
      "source": [
        "# Initialize VADER\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to calculate sentiment score\n",
        "def calculate_sentiment(remark):\n",
        "    scores = sid.polarity_scores(remark)\n",
        "    compound_score = scores['compound']\n",
        "    return compound_score\n",
        "\n",
        "# Imputing mode in customer_remarks\n",
        "df['customer_remarks'] = df['customer_remarks'].fillna('Neutral')\n",
        "# Apply sentiment calculation\n",
        "df['sentiment'] = df['customer_remarks'].apply(calculate_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Mx0v3TA1kWAr"
      },
      "outputs": [],
      "source": [
        "# Creating features based on issue_responded, issue_reported_at and survey_response_date\n",
        "df['wait_response_time'] = (df['issue_responded'] - df['issue_reported_at']).dt.total_seconds()/60\n",
        "\n",
        "# Note : survey_response date and issue_responded on same date for all rows.\n",
        "\n",
        "# Calculate mean of Wait_response_time where it's greater than 0\n",
        "mean_wait_response_time = df[df['wait_response_time'] > 0]['wait_response_time'].mean()\n",
        "# Impute negative values with mean\n",
        "df['wait_response_time'] = df['wait_response_time'].apply(lambda x: mean_wait_response_time if x < 0 else x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5K44XEapmva"
      },
      "source": [
        "### 2. Drop Unnecesary columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeGrUrsR85gv",
        "outputId": "4c6143ac-dc7f-4c1c-f7e0-a28028845a9f"
      },
      "outputs": [],
      "source": [
        "# Drop unnecessary columns and columns with very high percentage of nulls\n",
        "# Drop columns which are not useful because of high number of missing values or high number of categories\n",
        "drop_cols = ['order_date_time', 'customer_city',\t'product_category',\n",
        "           'item_price',\t'connected_handling_time', 'customer_remarks', 'issue_reported_at',\t'issue_responded',\n",
        "             'survey_response_date']\n",
        "df = df.drop(columns=drop_cols)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lets see if we have any duplicate rows\n",
        "df = check_drop_duplications(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Columns with high null% and unuseful columns for the csat score prediction are dropped."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id1riN9m0vUs"
      },
      "source": [
        "### 3. Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create directory for pickle files if it doesn't exist\n",
        "pkl_folder = 'pkl_files'\n",
        "os.makedirs(pkl_folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxgQI6Ho_Yjq"
      },
      "outputs": [],
      "source": [
        "#Reset index\n",
        "df.reset_index(inplace = True, drop=True)\n",
        "\n",
        "# Columns for one-hot encoding\n",
        "columns_for_one_hot_encoding = ['channel_name', 'category', 'tenure_bucket', 'agent_shift']\n",
        "\n",
        "# Initialize the OneHotEncoder\n",
        "one_hot_encoder = OneHotEncoder(sparse=False, drop='first', dtype=int)\n",
        "\n",
        "# Fit and transform the data\n",
        "one_hot_encoded = one_hot_encoder.fit_transform(df[columns_for_one_hot_encoding])\n",
        "\n",
        "# Convert the result into a DataFrame\n",
        "one_hot_encoded_df = pd.DataFrame(one_hot_encoded, columns=one_hot_encoder.get_feature_names_out(columns_for_one_hot_encoding))\n",
        "\n",
        "# Combine the one-hot encoded columns with the original DataFrame (excluding the original categorical columns)\n",
        "df = pd.concat([df.drop(columns=columns_for_one_hot_encoding), one_hot_encoded_df], axis=1)\n",
        "\n",
        "# Save the one-hot encoder to a pickle file\n",
        "one_hot_encoder_path = os.path.join(pkl_folder, 'one_hot_encoder.pkl')\n",
        "with open(one_hot_encoder_path, 'wb') as f:\n",
        "    pickle.dump(one_hot_encoder, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN4bCOotJQQm"
      },
      "outputs": [],
      "source": [
        "# Assuming the DataFrame is already loaded as 'df'\n",
        "# Ensure there are no NaN values initially\n",
        "assert df[['sub-category', 'agent_name', 'supervisor', 'manager']].isna().sum().sum() == 0, \"There are NaN values initially!\"\n",
        "\n",
        "# Columns for target encoding\n",
        "columns_for_target_encoding = ['sub-category', 'agent_name', 'supervisor', 'manager']\n",
        "\n",
        "# Perform target encoding\n",
        "target_encoders = {}\n",
        "for col in columns_for_target_encoding:\n",
        "    scaler = TargetEncoder()\n",
        "    df[col] = scaler.fit_transform(X=df[col], y=df['csat_score'])\n",
        "    target_encoders[col] = scaler\n",
        "\n",
        "# Check if any NaN values appeared after encoding\n",
        "if df[columns_for_target_encoding].isna().sum().sum() > 0:\n",
        "    print(\"Warning: NaN values found after target encoding. Consider imputing or handling them.\")\n",
        "else:\n",
        "    print(\"Target encoding applied successfully without introducing NaN values.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Data Encoding Overview\n",
        "\n",
        "For the preprocessing of categorical variables in our dataset, we employed two types of encoding techniques:\n",
        "\n",
        "1. **One-Hot Encoding:**\n",
        "   - Applied to the columns: `channel_name`, `category`, `tenure_bucket`, and `agent_shift`.\n",
        "   - The one-hot encoded features were stored using `OneHotEncoder` from `sklearn` and saved in a pickle file named `one_hot_encoder.pkl`.\n",
        "\n",
        "2. **Target Encoding:**\n",
        "   - Applied to the columns: `sub-category`, `agent_name`, `supervisor`, and `manager`.\n",
        "   - The target encoders were created using `TargetEncoder` from the `category_encoders` library and saved in a pickle file named `target_encoders.pkl`.\n",
        "\n",
        "These pickle files ensure that the same encodings can be consistently applied to future datasets, such as test data, maintaining consistency and accuracy across the model pipeline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNVZ9zx19K6k"
      },
      "source": [
        "### 4. Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGAENNp3pqLS"
      },
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "X = df.drop(columns=['csat_score'])\n",
        "y = df['csat_score'].values.reshape(-1, 1)  # Reshape y to a 2D array\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the scaler for both X and y\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "\n",
        "# Save the scalers for future use\n",
        "with open('pkl_files/scaler_X.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler_X, f)\n",
        "    \n",
        "with open('pkl_files/scaler_y.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler_y, f)\n",
        "\n",
        "# Convert the scaled data back to DataFrame if needed\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "y_train_scaled = pd.DataFrame(y_train_scaled, columns=['csat_score'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Scaling the Training Data\n",
        "\n",
        "To ensure consistent feature scaling across the dataset, we applied the `MinMaxScaler` from `sklearn.preprocessing`. The scaling process involves the following steps:\n",
        "\n",
        "1. **Fitting the Scaler on Training Data:**\n",
        "   - The `MinMaxScaler` was fitted on the complete data. This process calculates the minimum and maximum values for each feature dataset.\n",
        "   - These values are used to scale the features within a range of [0, 1], ensuring that each feature contributes proportionally to the model.\n",
        "\n",
        "2. **Transforming the Training Data:**\n",
        "   - After fitting, the scaler was used to transform the data, applying the calculated scaling parameters (min and max values) to each feature. This transformation normalizes the data, which helps in improving the convergence of gradient-based optimization algorithms used in many machine learning models.\n",
        "\n",
        "3. **Saving the Scaler:**\n",
        "   - The fitted scaler was saved to a pickle file (`min_max_scaler.pkl`). This ensures that the same scaling parameters can be consistently applied to the test data and any future datasets, maintaining the integrity of the model's performance.\n",
        "\n",
        "By following this approach, we ensure that the model is trained on appropriately scaled data, while preventing any data leakage from the test set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-rWR975dqjA"
      },
      "source": [
        "## ***5.Modelling & Evaluation***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JU2MNYeFfGV"
      },
      "outputs": [],
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_scaled, y_train_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Assuming X_train and X_val are your feature matrices\n",
        "input_dim = X_train.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Neural Network Architecture Overview\n",
        "\n",
        "The neural network model is designed to predict CSAT (Customer Satisfaction) scores using a simple yet effective architecture. The key components of this model are as follows:\n",
        "\n",
        "1. **Input Layer:**\n",
        "   - The model begins with an input layer that accepts features from the dataset. The number of input neurons corresponds to the number of features in the input data (`input_dim`).\n",
        "\n",
        "2. **First Hidden Layer:**\n",
        "   - A dense (fully connected) layer with 64 neurons is used, employing the ReLU (Rectified Linear Unit) activation function. This layer captures non-linear relationships within the data.\n",
        "   - A dropout layer follows this dense layer, with a dropout rate of 20% (`0.2`). This technique helps prevent overfitting by randomly setting 20% of the layer's output to zero during training.\n",
        "\n",
        "3. **Second Hidden Layer:**\n",
        "   - Another dense layer with 64 neurons, again using the ReLU activation function. This additional layer allows the model to learn more complex patterns from the data.\n",
        "   - This is followed by another dropout layer, also with a dropout rate of 20%.\n",
        "\n",
        "4. **Output Layer:**\n",
        "   - The output layer consists of a single neuron with a linear activation function. This is suitable for regression tasks, such as predicting a continuous value like the CSAT score.\n",
        "\n",
        "5. **Compilation:**\n",
        "   - The model is compiled using the Adam optimizer, with a learning rate of `0.001`. Adam is an adaptive learning rate optimization algorithm that combines the best properties of the AdaGrad and RMSProp algorithms, making it well-suited for this task.\n",
        "   - The loss function is `mean_squared_error`, which is appropriate for regression problems, as it measures the average squared difference between the predicted and actual values.\n",
        "   - The model's performance is tracked using the `accuracy` metric, although for regression, this may represent a custom or adjusted interpretation of accuracy.\n",
        "\n",
        "6. **Early Stopping:**\n",
        "   - To prevent overfitting and ensure that the model retains the best weights during training, early stopping is employed. The training process is monitored on the validation loss, and if no improvement is seen for 5 consecutive epochs, training is stopped, and the best model weights are restored.\n",
        "\n",
        "This architecture balances simplicity and power, making it suitable for predicting CSAT scores while incorporating regularization techniques to avoid overfitting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFOa2Y_--s3c"
      },
      "outputs": [],
      "source": [
        "# Model Architecture\n",
        "# Input layer > Dropout > Hidden layer with relu activation > Dropout > Output layer\n",
        "# Defined parameters -> neurons = 64, dropout rate = 0.2, learning rate = 0.001\n",
        "\n",
        "def create_model(neurons=64, dropout_rate=0.2, learning_rate=0.001):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(neurons, input_shape=(input_dim,), activation='relu'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(neurons, activation='relu'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NgwRk074eR76",
        "outputId": "0aed7c1d-bd74-411c-c53a-3e89b06b4880"
      },
      "outputs": [],
      "source": [
        "# Create the model with defined parameters\n",
        "model_v0 = create_model()\n",
        "\n",
        "# Train the model\n",
        "history_v0 = model_v0.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model_v0.evaluate(X_val, y_val)\n",
        "print(f'Mean Absolute Error: {accuracy}')\n",
        "print(f'Mean Squared Error: {loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_KCkUNZVXpS",
        "outputId": "2849d2fc-538a-46cb-e263-a5a7ab52f563"
      },
      "outputs": [],
      "source": [
        "model_v0.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "y5FkajaqD93o",
        "outputId": "b864e49a-79ab-4dfb-d9dd-612032e605cb"
      },
      "outputs": [],
      "source": [
        "# Plotting loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_v0.history['loss'], label='Training Loss')\n",
        "plt.plot(history_v0.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plotting accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_v0.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_v0.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the best model in HDF5 format (common format for Keras models)\n",
        "model_v0.save('models_saved/model_v0.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 2 - Decaying learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def create_model_v1(neurons=64, dropout_rate=0.2, initial_learning_rate=0.01, decay_steps=1000, decay_rate=0.96):\n",
        "    input_dim = X_train.shape[1]  # Assuming X_train is your training data\n",
        "    \n",
        "    # Adjusted learning rate schedule with slower decay\n",
        "    lr_schedule = ExponentialDecay(\n",
        "        initial_learning_rate=initial_learning_rate,\n",
        "        decay_steps=decay_steps,\n",
        "        decay_rate=decay_rate,\n",
        "        staircase=False\n",
        "    )\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Dense(neurons, input_shape=(input_dim,), activation='relu'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(neurons, activation='relu'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    \n",
        "    model.compile(optimizer=Adam(learning_rate=lr_schedule), loss='mean_squared_error', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the model with defined parameters\n",
        "model_v1 = create_model_v1()\n",
        "\n",
        "# Train the model\n",
        "history_v1 = model_v1.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model_v1.evaluate(X_val, y_val)\n",
        "print(f'Mean Absolute Error: {accuracy}')\n",
        "print(f'Mean Squared Error: {loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotting loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_v1.history['loss'], label='Training Loss')\n",
        "plt.plot(history_v1.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plotting accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_v1.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_v1.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Decaying Learning Rate Benefits\n",
        "\n",
        "A decaying learning rate gradually reduces the step size during training, allowing the model to make finer adjustments as it converges. This often leads to faster convergence by reducing the number of epochs needed without sacrificing accuracy. In this case, applying a decaying learning rate helped decrease the total training time by lowering the number of epochs, while maintaining the same level of accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the best model in HDF5 format (common format for Keras models)\n",
        "model_v1.save('models_saved/model_v1.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 3 - Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "siwx15WWpCXI",
        "outputId": "f346e16a-f748-4ebd-f2e3-27a227283f22"
      },
      "outputs": [],
      "source": [
        "# Wrap the model using KerasRegressor\n",
        "model = KerasRegressor(build_fn=create_model, verbose=0)\n",
        "\n",
        "# Define the random search parameters with scaled uniform distributions\n",
        "param_dist = {\n",
        "    'neurons': randint(32, 129),  # Random integers between 32 and 128 inclusive\n",
        "    'dropout_rate': uniform(0.1, 0.4),  # Uniform distribution from 0.1 to 0.5\n",
        "    'batch_size': randint(16, 65),  # Random integers between 16 and 64 inclusive\n",
        "    'epochs': randint(50, 151),  # Random integers between 50 and 150 inclusive\n",
        "    'learning_rate': uniform(0.001, 0.009)  # Uniform distribution from 0.001 to 0.01\n",
        "}\n",
        "\n",
        "# Perform random search\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=4, cv=5, verbose=2, n_jobs=-1)\n",
        "\n",
        "# Fit the model with random search\n",
        "random_search_result = random_search.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=[early_stopping, print_epoch_callback,  store_history_callback])\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best: %f using %s\" % (random_search_result.best_score_, random_search_result.best_params_))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model Overview and Hyperparameter Tuning\n",
        "\n",
        "This model is a neural network designed for regression tasks, specifically to predict continuous target values. The architecture consists of two hidden layers, each with ReLU activation and dropout for regularization. The output layer uses a linear activation function, suitable for regression. The model is compiled with the Adam optimizer and mean squared error as the loss function.\n",
        "\n",
        "**Hyperparameter Tuning:**\n",
        "- A `RandomizedSearchCV` approach is used to optimize key hyperparameters, including the number of neurons, dropout rate, batch size, epochs, and learning rate.\n",
        "- Custom callbacks are employed to log training progress (`PrintEpochCallback`) and store training history (`StoreHistoryCallback`).\n",
        "- Validation data and callbacks are passed during the training process to ensure robust evaluation and to prevent overfitting with early stopping.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwSySp5m8Pjm",
        "outputId": "5206783e-1697-4f22-f838-eaa3c6ead0c3"
      },
      "outputs": [],
      "source": [
        "# Extract the best model\n",
        "best_model = random_search_result.best_estimator_.model\n",
        "\n",
        "# Evaluate the model to get the custom accuracy\n",
        "loss, accuracy = best_model.evaluate(X_val, y_val)\n",
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# Print the model summary\n",
        "best_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "lGn1FRzNE8US",
        "outputId": "64aa5d06-39a1-4c07-ea07-b77712d9cd3c"
      },
      "outputs": [],
      "source": [
        "# Plotting the training history\n",
        "history_dict = store_history_callback.history\n",
        "\n",
        "# Plot the training and validation loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_dict['loss'], label='Training loss')\n",
        "plt.plot(history_dict['val_loss'], label='Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "# Plot the training and validation custom accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_dict['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_dict['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the best model in HDF5 format (common format for Keras models)\n",
        "best_model.save('models_saved/best_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW23w6HcUR-0"
      },
      "source": [
        "## ***6.Predictions***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tranform test data and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Transform the test data using the same scaler\n",
        "X_test = scaler_X.transform(X_test)\n",
        "y_test = scaler_y.transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model to get the accuracy\n",
        "loss, accuracy = best_model.evaluate(X_test, y_test)\n",
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visulaization actuals vs predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "QSVCMGtzQvgr",
        "outputId": "98cb3916-2826-408b-e555-8edab3790bd9"
      },
      "outputs": [],
      "source": [
        "# Generate predictions for the validation set\n",
        "y_pred = best_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dhKxVVYQRjfy",
        "outputId": "dd0cabd0-865b-4f38-9251-fa45c233c4c5"
      },
      "outputs": [],
      "source": [
        "min_y = 1  # Replace with your original min value of y\n",
        "max_y = 5  # Replace with your original max value of y\n",
        "\n",
        "# Reverse scaling using the formula\n",
        "y_test_real = y_test * (max_y - min_y) + min_y\n",
        "y_pred_real = (y_pred * (max_y - min_y) + min_y).round()\n",
        "\n",
        "# Compare predictions to actual values\n",
        "comparison_df = pd.DataFrame({'Actual': y_test_real.flatten(), 'Predicted': y_pred_real.flatten()})\n",
        "comparison_df['Difference'] = comparison_df['Actual'] - comparison_df['Predicted']\n",
        "comparison_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the percentage of each difference value\n",
        "difference_percentage = comparison_df['Difference'].value_counts(normalize=True) * 100\n",
        "\n",
        "# Convert to DataFrame for easier plotting\n",
        "difference_percentage_df = difference_percentage.reset_index()\n",
        "difference_percentage_df.columns = ['Difference', 'Percentage']\n",
        "\n",
        "# Sort the differences for better plotting\n",
        "difference_percentage_df = difference_percentage_df.sort_values('Difference')\n",
        "\n",
        "# Plot the distribution of differences in percentages\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x='Difference', y='Percentage', data=difference_percentage_df, palette='viridis')\n",
        "\n",
        "plt.xlabel('Difference (Actual - Predicted)')\n",
        "plt.ylabel('Percentage (%)')\n",
        "plt.title('Distribution of Prediction Differences (Percentage)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This plot will visually represent how frequently the modelâ€™s predictions differ from the actual values by specific amounts, expressed as a percentage of the total predictions. This is helpful to understand the model's accuracy and error distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXrW0I2XawHh"
      },
      "source": [
        "## ***7.Conclusion*** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The development of the CSAT prediction model involved several critical steps, including data preparation, feature engineering, and model development. The model was designed to predict customer satisfaction scores based on key factors such as response times, sentiment analysis, and customer interactions. By accurately forecasting these scores, the model provides valuable insights into customer behavior and areas where service improvements can be made.\n",
        "\n",
        "The model has successfully identified key drivers of customer satisfaction and demonstrated its potential to predict CSAT scores with a high degree of accuracy. The analysis revealed that factors like response times and the sentiment of customer reviews play a significant role in determining satisfaction levels. While the model performs well overall, some discrepancies between predicted and actual scores highlight opportunities for further refinement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Action Points\n",
        "\n",
        "1. **Optimize Customer Service Operations:**\n",
        "\n",
        "   - **Reduce Response Times:** Implement strategies to minimize customer wait times, as this has been shown to significantly impact satisfaction. Consider increasing staff during peak periods or leveraging automation to improve response efficiency.\n",
        "\n",
        "   - **Monitor and Improve Customer Sentiment:** Use the insights from sentiment analysis to proactively address negative feedback and reinforce positive experiences. This could involve targeted training for customer service representatives or adjusting service protocols.\n",
        "\n",
        "2. **Enhance Data Quality and Model Accuracy:**\n",
        "\n",
        "   - **Refine Feature Engineering:** Continuously improve the model by exploring additional features that may contribute to more accurate predictions. Regularly update the dataset to reflect the most recent customer interactions and trends.\n",
        "\n",
        "   - **Address Prediction Outliers:** Investigate cases where the model's predictions deviate significantly from actual scores to identify potential service gaps or areas for process improvement.\n",
        "\n",
        "3. **Deploy Model for Proactive Customer Management:**\n",
        "\n",
        "   - **Real-Time Monitoring:** Integrate the model into customer service operations to provide real-time predictions that can guide decision-making and improve customer satisfaction proactively.\n",
        "   \n",
        "   - **Strategic Use of Insights:** Leverage the modelâ€™s predictions to inform broader business strategies, such as resource allocation, training programs, and service enhancements, to drive continuous improvement in customer satisfaction.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "rKaaJyKJ5dUD",
        "aoREz6ymV5Jr",
        "g-ATYxFrGrvw",
        "7wuGOrhz0itI",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
